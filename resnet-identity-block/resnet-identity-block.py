import numpy as np

def relu(x):
    return np.maximum(0, x)

class IdentityBlock:
    """
    Identity Block: F(x) + x
    Used when input and output dimensions match.
    """
    
    def __init__(self, channels: int):
        self.channels = channels
        # Simplified: using dense layers instead of conv for demo
        self.W1 = np.random.randn(channels, channels) * 0.01
        self.W2 = np.random.randn(channels, channels) * 0.01
    
    def forward(self, x: np.ndarray) -> np.ndarray:
        """
        Forward pass: y = ReLU(W2 @ ReLU(W1 @ x)) + x
        """
        self.y = np.matmul(self.W1, np.transpose(x))
        self.y = relu(self.y)
        self.y = np.matmul(self.W2, self.y)
        self.y = relu(self.y)
        self.y = self.y + np.transpose(x)

        # Transpose back to the orginal shape of the matrix before returning
        return np.transpose(self.y)
        







        